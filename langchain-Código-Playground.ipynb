{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatbot com memoria e template com variaveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar a chave de API do Google\n",
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "_ = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biblioteca para carregar o modelo da LLM\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# biblioteca para enviar mensagens para o modelo como humano\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# bibliotecas para adicionar hist√≥rico de mensagens ao chatbot\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o Modelo da LLM\n",
    "model = init_chat_model(\"gemini-2.0-flash-lite\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biblioteca para passar templates de contexto\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Voc√™ √© um assistente √∫til. Responda a todas as perguntas da melhor forma poss√≠vel em {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bibliotecas para criar uma classe de estado e enviar vari√°veis para o chatbot\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"thread_id\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iai, meu rei! Tudo sussa por aqui, e contigo? Pode falar, o que tu precisa? ü§†'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = \"Iai macho, tudo suave?\"\n",
    "language = \"Portugu√™s Cearense\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages, \"language\": language}, config)\n",
    "\n",
    "output[\"messages\"][-1].content  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Iai macho, tudo suave?', additional_kwargs={}, response_metadata={}, id='1564d955-3307-46fb-865f-b03b3e9ef777'),\n",
       "  AIMessage(content='Iai, meu rei! Tudo sussa por aqui, e contigo? Pode falar, o que tu precisa? ü§†', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-88c5c82e-1145-4356-b54a-702abe11a954-0', usage_metadata={'input_tokens': 28, 'output_tokens': 26, 'total_tokens': 54, 'input_token_details': {'cache_read': 0}})],\n",
       " 'language': 'Portugu√™s Cearense'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Transcreva o audio para texto\"\n",
    "\n",
    "# Lendo o conte√∫do do arquivo local\n",
    "with open(\"204.wav\", \"rb\") as audio_file:\n",
    "    audio_data = audio_file.read()\n",
    "\n",
    "input_messages = HumanMessage(\n",
    "    content=[\n",
    "        {\n",
    "            \"type\": \"text\",\t\n",
    "            \"text\": query,\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"media\",\n",
    "            \"data\": audio_data,  # Usando o conte√∫do do arquivo\n",
    "            \"mime_type\": \"audio/wav\",  # Adicionando o mime_type\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "output = model.invoke([input_messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A primeira maior guerra de todas foi entre o bem e o mal, o c√©u e a terra.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Transcreva o audio para texto e cite de onde √© a origem desse texto\"\n",
    "\n",
    "# Lendo o conte√∫do do arquivo local\n",
    "with open(\"206.wav\", \"rb\") as audio_file:\n",
    "    audio_data = audio_file.read()\n",
    "\n",
    "input_messages = HumanMessage(\n",
    "    content=[\n",
    "        {\n",
    "            \"type\": \"text\",\t\n",
    "            \"text\": query,\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"media\",\n",
    "            \"data\": audio_data,  # Usando o conte√∫do do arquivo\n",
    "            \"mime_type\": \"audio/wav\",  # Adicionando o mime_type\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "output2 = model.invoke([input_messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "O √°udio cont√©m a frase: \"Eu prefiro ser essa metamorfose ambulante.\"\n",
      "\n",
      "A origem dessa frase √© a m√∫sica \"Metamorfose Ambulante\" de Raul Seixas, lan√ßada em 1973.\n"
     ]
    }
   ],
   "source": [
    "output2.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
